{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05a5d19-8cc1-44b5-a6ed-27c8acffee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "应变窗口形状: (1278, 2048, 10)\n",
      "载荷窗口形状: (1278, 2048, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/base/mambaforge/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/base/mambaforge/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/base/mambaforge/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/base/mambaforge/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/base/mambaforge/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/base/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1459/229583466.py\", line 147, in <module>\n",
      "    train_dataset = StrainLoadTimeDomainDataset(X_train, y_train)\n",
      "  File \"/tmp/ipykernel_1459/229583466.py\", line 129, in __init__\n",
      "    self.strain = torch.tensor(strain_windows, dtype=torch.float32)  # 形状: (num_windows, 2048, 10)\n",
      "/tmp/ipykernel_1459/229583466.py:129: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  self.strain = torch.tensor(strain_windows, dtype=torch.float32)  # 形状: (num_windows, 2048, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import joblib\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# 设置中文字体（这里以SimHei字体为例）\n",
    "rcParams['font.sans-serif'] = ['SimHei']  # 指定中文字体\n",
    "rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 1. 数据加载与预处理\n",
    "def load_mat_files_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    从指定目录加载所有.mat文件，并合并load_data和strain_data。\n",
    "    \n",
    "    参数:\n",
    "        directory_path (str): .mat文件所在的目录路径。\n",
    "        \n",
    "    返回:\n",
    "        load_list (list): 所有.mat文件中的load_data列表。\n",
    "        strain_list (list): 所有.mat文件中的strain_data列表。\n",
    "    \"\"\"\n",
    "    load_list = []\n",
    "    strain_list = []\n",
    "    \n",
    "    # 遍历目录中的所有文件\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.mat'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            mat = scipy.io.loadmat(file_path)\n",
    "            \n",
    "            # 提取数据\n",
    "            current_load = mat.get('load_data')\n",
    "            current_strain = mat.get('strain_data')\n",
    "            \n",
    "            if current_load is None or current_strain is None:\n",
    "                print(f\"文件 {filename} 缺少 'load_data' 或 'strain_data'，已跳过。\")\n",
    "                continue\n",
    "            \n",
    "            load_list.append(current_load)\n",
    "            strain_list.append(current_strain)\n",
    "    \n",
    "    if not load_list or not strain_list:\n",
    "        raise ValueError(\"未找到任何包含 'load_data' 和 'strain_data' 的.mat文件。\")\n",
    "    \n",
    "    # 合并所有加载的数据\n",
    "    combined_load = np.concatenate(load_list, axis=1)  # 假设第二维是样本数\n",
    "    combined_strain = np.concatenate(strain_list, axis=1)\n",
    "    \n",
    "    return combined_load, combined_strain\n",
    "\n",
    "# 指定包含多个.mat文件的文件夹路径\n",
    "mat_directory = r'../../shared-nvme/PythonG4090/data'  # 修改为您的文件夹路径\n",
    "\n",
    "# 从文件夹中加载所有.mat文件的数据\n",
    "combined_load_data, combined_strain_data = load_mat_files_from_directory(mat_directory)\n",
    "\n",
    "# 转置数据，使时间为第一个维度\n",
    "strain = combined_strain_data.T  # 形状: (总样本数, 10)\n",
    "load = combined_load_data.T      # 形状: (总样本数, 1)\n",
    "\n",
    "# 确保 load 是二维的，形状为 (样本数, 1)\n",
    "if load.ndim > 2:\n",
    "    load = load.squeeze()\n",
    "if load.ndim == 1:\n",
    "    load = load.reshape(-1, 1)\n",
    "elif load.shape[1] != 1:\n",
    "    load = load[:, 0:1]\n",
    "\n",
    "# 标准化数据\n",
    "strain_scaler = StandardScaler()\n",
    "load_scaler = StandardScaler()\n",
    "\n",
    "strain = strain_scaler.fit_transform(strain)  # 形状: (总样本数, 10)\n",
    "load = load_scaler.fit_transform(load)        # 形状: (总样本数, 1)\n",
    "\n",
    "# 保存scalers，以便后续使用\n",
    "joblib.dump(strain_scaler, 'strain_scaler.save')\n",
    "joblib.dump(load_scaler, 'load_scaler.save')\n",
    "\n",
    "# 2. 数据切片\n",
    "def create_time_windows(data, window_size=2048, overlap=0.5):\n",
    "    \"\"\"\n",
    "    将数据切分成时间窗口。\n",
    "    \n",
    "    参数:\n",
    "        data (numpy.ndarray): 输入数据，形状为 (样本数, 特征数)。\n",
    "        window_size (int): 每个窗口的大小。\n",
    "        overlap (float): 窗口之间的重叠比例。\n",
    "        \n",
    "    返回:\n",
    "        windows (numpy.ndarray): 切分后的数据，形状为 (窗口数, window_size, 特征数)。\n",
    "    \"\"\"\n",
    "    step = int(window_size * (1 - overlap))\n",
    "    num_samples = data.shape[0]\n",
    "    num_features = data.shape[1]\n",
    "    \n",
    "    num_windows = (num_samples - window_size) // step + 1\n",
    "    windows = np.zeros((num_windows, window_size, num_features))\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start = i * step\n",
    "        end = start + window_size\n",
    "        windows[i] = data[start:end]\n",
    "    \n",
    "    return windows\n",
    "\n",
    "# 定义窗口大小和重叠比例\n",
    "window_size = 2048\n",
    "overlap = 0.5\n",
    "\n",
    "# 切片应变数据和载荷数据\n",
    "strain_windows = create_time_windows(strain, window_size=window_size, overlap=overlap)  # 形状: (num_windows, 2048, 10)\n",
    "load_windows = create_time_windows(load, window_size=window_size, overlap=overlap)        # 形状: (num_windows, 2048, 1)\n",
    "\n",
    "print(f'应变窗口形状: {strain_windows.shape}')  # 例如: (num_windows, 2048, 10)\n",
    "print(f'载荷窗口形状: {load_windows.shape}')    # 例如: (num_windows, 2048, 1)\n",
    "\n",
    "# 3. 创建数据集\n",
    "class StrainLoadTimeDomainDataset(Dataset):\n",
    "    def __init__(self, strain_windows, load_windows):\n",
    "        self.strain = torch.tensor(strain_windows, dtype=torch.float32)  # 形状: (num_windows, 2048, 10)\n",
    "        self.load = torch.tensor(load_windows, dtype=torch.float32)      # 形状: (num_windows, 2048, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.strain.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回应变序列和载荷序列\n",
    "        strain_sample = self.strain[idx]  # 形状: (2048, 10)\n",
    "        load_sample = self.load[idx]      # 形状: (2048, 1)\n",
    "        return strain_sample, load_sample\n",
    "\n",
    "# 拆分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    strain_windows, load_windows, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建数据集对象\n",
    "train_dataset = StrainLoadTimeDomainDataset(X_train, y_train)\n",
    "val_dataset = StrainLoadTimeDomainDataset(X_val, y_val)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e44973a-d429-4587-b3ff-3d7141b180c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Loss: 0.985093, Val Loss: 0.935796\n",
      "Epoch [2/300], Train Loss: 0.905992, Val Loss: 0.840978\n",
      "Epoch [3/300], Train Loss: 0.810242, Val Loss: 0.769283\n",
      "Epoch [4/300], Train Loss: 0.783066, Val Loss: 0.769197\n",
      "Epoch [5/300], Train Loss: 0.732946, Val Loss: 0.718743\n",
      "Epoch [6/300], Train Loss: 0.712769, Val Loss: 0.780821\n",
      "Epoch [7/300], Train Loss: 0.733789, Val Loss: 0.708863\n",
      "Epoch [8/300], Train Loss: 0.688339, Val Loss: 0.721906\n",
      "Epoch [9/300], Train Loss: 0.661975, Val Loss: 0.662260\n",
      "Epoch [10/300], Train Loss: 0.643572, Val Loss: 0.618004\n",
      "Epoch [11/300], Train Loss: 0.606397, Val Loss: 0.642458\n",
      "Epoch [12/300], Train Loss: 0.623073, Val Loss: 0.600979\n",
      "Epoch [13/300], Train Loss: 0.594586, Val Loss: 0.624365\n",
      "Epoch [14/300], Train Loss: 0.577726, Val Loss: 0.546674\n",
      "Epoch [15/300], Train Loss: 0.540072, Val Loss: 0.568092\n",
      "Epoch [16/300], Train Loss: 0.544303, Val Loss: 0.522844\n",
      "Epoch [17/300], Train Loss: 0.549366, Val Loss: 0.583708\n",
      "Epoch [18/300], Train Loss: 0.515229, Val Loss: 0.514039\n",
      "Epoch [19/300], Train Loss: 0.495446, Val Loss: 0.495055\n",
      "Epoch [20/300], Train Loss: 0.465921, Val Loss: 0.483968\n",
      "Epoch [21/300], Train Loss: 0.442814, Val Loss: 0.444876\n",
      "Epoch [22/300], Train Loss: 0.451593, Val Loss: 0.460217\n",
      "Epoch [23/300], Train Loss: 0.417287, Val Loss: 0.424889\n",
      "Epoch [24/300], Train Loss: 0.396888, Val Loss: 0.397973\n",
      "Epoch [25/300], Train Loss: 0.380345, Val Loss: 0.398785\n",
      "Epoch [26/300], Train Loss: 0.381488, Val Loss: 0.424286\n",
      "Epoch [27/300], Train Loss: 0.378583, Val Loss: 0.391770\n",
      "Epoch [28/300], Train Loss: 0.433603, Val Loss: 0.549621\n",
      "Epoch [29/300], Train Loss: 0.416596, Val Loss: 0.388933\n",
      "Epoch [30/300], Train Loss: 0.338272, Val Loss: 0.375762\n",
      "Epoch [31/300], Train Loss: 0.299844, Val Loss: 0.326469\n",
      "Epoch [32/300], Train Loss: 0.272820, Val Loss: 0.314196\n",
      "Epoch [33/300], Train Loss: 0.250573, Val Loss: 0.291006\n",
      "Epoch [34/300], Train Loss: 0.229525, Val Loss: 0.281060\n",
      "Epoch [35/300], Train Loss: 0.213564, Val Loss: 0.270151\n",
      "Epoch [36/300], Train Loss: 0.201519, Val Loss: 0.249872\n",
      "Epoch [37/300], Train Loss: 0.186566, Val Loss: 0.241389\n",
      "Epoch [38/300], Train Loss: 0.171707, Val Loss: 0.233450\n",
      "Epoch [39/300], Train Loss: 0.165164, Val Loss: 0.224946\n",
      "Epoch [40/300], Train Loss: 0.153017, Val Loss: 0.216357\n",
      "Epoch [41/300], Train Loss: 0.140705, Val Loss: 0.203540\n",
      "Epoch [42/300], Train Loss: 0.126997, Val Loss: 0.191267\n",
      "Epoch [43/300], Train Loss: 0.118310, Val Loss: 0.185399\n",
      "Epoch [44/300], Train Loss: 0.109877, Val Loss: 0.180934\n",
      "Epoch [45/300], Train Loss: 0.104264, Val Loss: 0.173011\n",
      "Epoch [46/300], Train Loss: 0.098712, Val Loss: 0.171305\n",
      "Epoch [47/300], Train Loss: 0.093615, Val Loss: 0.164895\n",
      "Epoch [48/300], Train Loss: 0.089609, Val Loss: 0.160503\n",
      "Epoch [49/300], Train Loss: 0.083913, Val Loss: 0.157252\n",
      "Epoch [50/300], Train Loss: 0.080731, Val Loss: 0.152919\n",
      "Epoch [51/300], Train Loss: 0.074624, Val Loss: 0.149666\n",
      "Epoch [52/300], Train Loss: 0.070990, Val Loss: 0.152026\n",
      "Epoch [53/300], Train Loss: 0.080260, Val Loss: 0.153348\n",
      "Epoch [54/300], Train Loss: 0.078401, Val Loss: 0.157414\n",
      "Epoch [55/300], Train Loss: 0.067440, Val Loss: 0.140127\n",
      "Epoch [56/300], Train Loss: 0.059843, Val Loss: 0.137651\n",
      "Epoch [57/300], Train Loss: 0.057107, Val Loss: 0.134617\n",
      "Epoch [58/300], Train Loss: 0.053919, Val Loss: 0.133019\n",
      "Epoch [59/300], Train Loss: 0.051535, Val Loss: 0.129789\n",
      "Epoch [60/300], Train Loss: 0.049390, Val Loss: 0.128460\n",
      "Epoch [61/300], Train Loss: 0.048029, Val Loss: 0.127137\n",
      "Epoch [62/300], Train Loss: 0.046830, Val Loss: 0.127516\n",
      "Epoch [63/300], Train Loss: 0.046041, Val Loss: 0.125939\n",
      "Epoch [64/300], Train Loss: 0.044624, Val Loss: 0.125456\n",
      "Epoch [65/300], Train Loss: 0.043425, Val Loss: 0.122729\n",
      "Epoch [66/300], Train Loss: 0.042290, Val Loss: 0.122719\n",
      "Epoch [67/300], Train Loss: 0.041179, Val Loss: 0.123546\n",
      "Epoch [68/300], Train Loss: 0.041547, Val Loss: 0.125162\n",
      "Epoch [69/300], Train Loss: 0.041404, Val Loss: 0.124587\n",
      "Epoch [70/300], Train Loss: 0.041562, Val Loss: 0.121054\n",
      "Epoch [71/300], Train Loss: 0.038679, Val Loss: 0.118834\n",
      "Epoch [72/300], Train Loss: 0.037086, Val Loss: 0.118173\n",
      "Epoch [73/300], Train Loss: 0.036143, Val Loss: 0.119151\n",
      "Epoch [74/300], Train Loss: 0.036898, Val Loss: 0.118818\n",
      "Epoch [75/300], Train Loss: 0.035802, Val Loss: 0.117689\n",
      "Epoch [76/300], Train Loss: 0.035230, Val Loss: 0.116996\n",
      "Epoch [77/300], Train Loss: 0.036114, Val Loss: 0.117937\n",
      "Epoch [78/300], Train Loss: 0.037016, Val Loss: 0.118545\n",
      "Epoch [79/300], Train Loss: 0.038523, Val Loss: 0.118564\n",
      "Epoch [80/300], Train Loss: 0.036183, Val Loss: 0.120010\n",
      "Epoch [81/300], Train Loss: 0.035762, Val Loss: 0.119511\n",
      "Epoch [82/300], Train Loss: 0.034788, Val Loss: 0.116520\n",
      "Epoch [83/300], Train Loss: 0.031692, Val Loss: 0.115948\n",
      "Epoch [84/300], Train Loss: 0.031055, Val Loss: 0.113021\n",
      "Epoch [85/300], Train Loss: 0.029920, Val Loss: 0.112444\n",
      "Epoch [86/300], Train Loss: 0.029128, Val Loss: 0.112836\n",
      "Epoch [87/300], Train Loss: 0.028391, Val Loss: 0.111152\n",
      "Epoch [88/300], Train Loss: 0.027284, Val Loss: 0.111537\n",
      "Epoch [89/300], Train Loss: 0.026606, Val Loss: 0.110283\n",
      "Epoch [90/300], Train Loss: 0.026379, Val Loss: 0.111466\n",
      "Epoch [91/300], Train Loss: 0.026216, Val Loss: 0.110161\n",
      "Epoch [92/300], Train Loss: 0.025883, Val Loss: 0.109670\n",
      "Epoch [93/300], Train Loss: 0.026154, Val Loss: 0.111871\n",
      "Epoch [94/300], Train Loss: 0.027490, Val Loss: 0.114057\n",
      "Epoch [95/300], Train Loss: 0.027464, Val Loss: 0.110393\n",
      "Epoch [96/300], Train Loss: 0.025813, Val Loss: 0.113117\n",
      "Epoch [97/300], Train Loss: 0.025954, Val Loss: 0.110641\n",
      "Epoch [98/300], Train Loss: 0.040182, Val Loss: 0.143304\n",
      "Epoch [99/300], Train Loss: 0.052090, Val Loss: 0.130516\n",
      "Epoch [100/300], Train Loss: 0.038072, Val Loss: 0.115418\n",
      "Epoch [101/300], Train Loss: 0.028781, Val Loss: 0.110820\n",
      "Epoch [102/300], Train Loss: 0.024940, Val Loss: 0.108455\n",
      "Epoch [103/300], Train Loss: 0.022929, Val Loss: 0.107562\n",
      "Epoch [104/300], Train Loss: 0.021839, Val Loss: 0.106430\n",
      "Epoch [105/300], Train Loss: 0.020952, Val Loss: 0.105373\n",
      "Epoch [106/300], Train Loss: 0.020392, Val Loss: 0.104867\n",
      "Epoch [107/300], Train Loss: 0.020047, Val Loss: 0.105682\n",
      "Epoch [108/300], Train Loss: 0.019954, Val Loss: 0.104817\n",
      "Epoch [109/300], Train Loss: 0.020177, Val Loss: 0.106072\n",
      "Epoch [110/300], Train Loss: 0.020088, Val Loss: 0.105140\n",
      "Epoch [111/300], Train Loss: 0.019595, Val Loss: 0.105138\n",
      "Epoch [112/300], Train Loss: 0.019426, Val Loss: 0.105218\n",
      "Epoch [113/300], Train Loss: 0.019249, Val Loss: 0.104975\n",
      "Epoch [114/300], Train Loss: 0.019127, Val Loss: 0.104574\n",
      "Epoch [115/300], Train Loss: 0.019495, Val Loss: 0.105715\n",
      "Epoch [116/300], Train Loss: 0.020039, Val Loss: 0.105211\n",
      "Epoch [117/300], Train Loss: 0.021103, Val Loss: 0.106846\n",
      "Epoch [118/300], Train Loss: 0.019860, Val Loss: 0.106017\n",
      "Epoch [119/300], Train Loss: 0.019811, Val Loss: 0.107022\n",
      "Epoch [120/300], Train Loss: 0.020901, Val Loss: 0.105097\n",
      "Epoch [121/300], Train Loss: 0.019404, Val Loss: 0.104343\n",
      "Epoch [122/300], Train Loss: 0.018368, Val Loss: 0.105342\n",
      "Epoch [123/300], Train Loss: 0.018365, Val Loss: 0.105436\n",
      "Epoch [124/300], Train Loss: 0.018292, Val Loss: 0.106630\n",
      "Epoch [125/300], Train Loss: 0.019562, Val Loss: 0.106107\n",
      "Epoch [126/300], Train Loss: 0.019690, Val Loss: 0.104176\n",
      "Epoch [127/300], Train Loss: 0.020761, Val Loss: 0.110470\n",
      "Epoch [128/300], Train Loss: 0.023506, Val Loss: 0.111107\n",
      "Epoch [129/300], Train Loss: 0.021747, Val Loss: 0.106052\n",
      "Epoch [130/300], Train Loss: 0.019019, Val Loss: 0.104465\n",
      "Epoch [131/300], Train Loss: 0.017838, Val Loss: 0.104183\n",
      "Epoch [132/300], Train Loss: 0.017789, Val Loss: 0.105545\n",
      "Epoch [133/300], Train Loss: 0.017843, Val Loss: 0.107108\n",
      "Epoch [134/300], Train Loss: 0.020380, Val Loss: 0.104382\n",
      "Epoch [135/300], Train Loss: 0.018080, Val Loss: 0.104783\n",
      "Epoch [136/300], Train Loss: 0.017032, Val Loss: 0.102350\n",
      "Epoch [137/300], Train Loss: 0.016465, Val Loss: 0.102178\n",
      "Epoch [138/300], Train Loss: 0.016995, Val Loss: 0.104158\n",
      "Epoch [139/300], Train Loss: 0.017085, Val Loss: 0.103353\n",
      "Epoch [140/300], Train Loss: 0.016273, Val Loss: 0.102197\n",
      "Epoch [141/300], Train Loss: 0.016775, Val Loss: 0.103828\n",
      "Epoch [142/300], Train Loss: 0.017034, Val Loss: 0.103560\n",
      "Epoch [143/300], Train Loss: 0.018098, Val Loss: 0.105478\n",
      "Epoch [144/300], Train Loss: 0.017084, Val Loss: 0.104134\n",
      "Epoch [145/300], Train Loss: 0.016374, Val Loss: 0.104761\n",
      "Epoch [146/300], Train Loss: 0.018971, Val Loss: 0.106617\n",
      "Epoch [147/300], Train Loss: 0.019688, Val Loss: 0.108073\n",
      "Epoch [148/300], Train Loss: 0.017334, Val Loss: 0.103975\n",
      "Epoch [149/300], Train Loss: 0.015648, Val Loss: 0.102399\n",
      "Epoch [150/300], Train Loss: 0.017067, Val Loss: 0.105916\n",
      "Epoch [151/300], Train Loss: 0.015999, Val Loss: 0.102619\n",
      "Epoch [152/300], Train Loss: 0.017480, Val Loss: 0.103037\n",
      "Epoch [153/300], Train Loss: 0.016448, Val Loss: 0.102635\n",
      "Epoch [154/300], Train Loss: 0.015187, Val Loss: 0.103142\n",
      "Epoch [155/300], Train Loss: 0.014709, Val Loss: 0.101122\n",
      "Epoch [156/300], Train Loss: 0.014385, Val Loss: 0.101933\n",
      "Epoch [157/300], Train Loss: 0.013769, Val Loss: 0.100651\n",
      "Epoch [158/300], Train Loss: 0.013502, Val Loss: 0.100437\n",
      "Epoch [159/300], Train Loss: 0.013597, Val Loss: 0.100346\n",
      "Epoch [160/300], Train Loss: 0.013501, Val Loss: 0.099868\n",
      "Epoch [161/300], Train Loss: 0.014352, Val Loss: 0.102204\n",
      "Epoch [162/300], Train Loss: 0.017040, Val Loss: 0.112354\n",
      "Epoch [163/300], Train Loss: 0.018545, Val Loss: 0.103902\n",
      "Epoch [164/300], Train Loss: 0.016047, Val Loss: 0.102278\n",
      "Epoch [165/300], Train Loss: 0.014861, Val Loss: 0.102144\n",
      "Epoch [166/300], Train Loss: 0.015197, Val Loss: 0.101500\n",
      "Epoch [167/300], Train Loss: 0.013590, Val Loss: 0.100363\n",
      "Epoch [168/300], Train Loss: 0.012837, Val Loss: 0.100143\n",
      "Epoch [169/300], Train Loss: 0.012451, Val Loss: 0.099490\n",
      "Epoch [170/300], Train Loss: 0.012607, Val Loss: 0.100401\n",
      "Epoch [171/300], Train Loss: 0.013541, Val Loss: 0.100999\n",
      "Epoch [172/300], Train Loss: 0.013753, Val Loss: 0.102300\n",
      "Epoch [173/300], Train Loss: 0.013304, Val Loss: 0.100110\n",
      "Epoch [174/300], Train Loss: 0.012648, Val Loss: 0.100898\n",
      "Epoch [175/300], Train Loss: 0.012280, Val Loss: 0.099710\n",
      "Epoch [176/300], Train Loss: 0.012283, Val Loss: 0.099789\n",
      "Epoch [177/300], Train Loss: 0.012348, Val Loss: 0.100827\n",
      "Epoch [178/300], Train Loss: 0.012480, Val Loss: 0.102082\n",
      "Epoch [179/300], Train Loss: 0.013251, Val Loss: 0.100844\n",
      "Epoch [180/300], Train Loss: 0.012727, Val Loss: 0.099586\n",
      "Epoch [181/300], Train Loss: 0.012949, Val Loss: 0.100028\n",
      "Epoch [182/300], Train Loss: 0.014360, Val Loss: 0.104801\n",
      "Epoch [183/300], Train Loss: 0.016734, Val Loss: 0.103838\n",
      "Epoch [184/300], Train Loss: 0.015098, Val Loss: 0.101833\n",
      "Epoch [185/300], Train Loss: 0.014164, Val Loss: 0.101358\n",
      "Epoch [186/300], Train Loss: 0.012640, Val Loss: 0.099281\n",
      "Epoch [187/300], Train Loss: 0.012051, Val Loss: 0.100088\n",
      "Epoch [188/300], Train Loss: 0.012102, Val Loss: 0.100247\n",
      "Epoch [189/300], Train Loss: 0.016336, Val Loss: 0.106779\n",
      "Epoch [190/300], Train Loss: 0.016942, Val Loss: 0.105630\n",
      "Epoch [191/300], Train Loss: 0.014321, Val Loss: 0.102248\n",
      "Epoch [192/300], Train Loss: 0.013352, Val Loss: 0.101017\n",
      "Epoch [193/300], Train Loss: 0.012055, Val Loss: 0.099004\n",
      "Epoch [194/300], Train Loss: 0.011462, Val Loss: 0.100456\n",
      "Epoch [195/300], Train Loss: 0.011551, Val Loss: 0.098603\n",
      "Epoch [196/300], Train Loss: 0.011777, Val Loss: 0.101338\n",
      "Epoch [197/300], Train Loss: 0.011632, Val Loss: 0.099657\n",
      "Epoch [198/300], Train Loss: 0.011261, Val Loss: 0.098301\n",
      "Epoch [199/300], Train Loss: 0.011048, Val Loss: 0.098126\n",
      "Epoch [200/300], Train Loss: 0.010820, Val Loss: 0.100112\n",
      "Epoch [201/300], Train Loss: 0.012290, Val Loss: 0.107049\n",
      "Epoch [202/300], Train Loss: 0.018330, Val Loss: 0.107850\n",
      "Epoch [203/300], Train Loss: 0.014523, Val Loss: 0.100973\n",
      "Epoch [204/300], Train Loss: 0.013418, Val Loss: 0.100116\n",
      "Epoch [205/300], Train Loss: 0.012386, Val Loss: 0.099415\n",
      "Epoch [206/300], Train Loss: 0.011363, Val Loss: 0.098683\n",
      "Epoch [207/300], Train Loss: 0.010466, Val Loss: 0.098611\n",
      "Epoch [208/300], Train Loss: 0.010399, Val Loss: 0.098704\n",
      "Epoch [209/300], Train Loss: 0.010936, Val Loss: 0.099039\n",
      "Epoch [210/300], Train Loss: 0.011174, Val Loss: 0.098870\n",
      "Epoch [211/300], Train Loss: 0.011770, Val Loss: 0.099783\n",
      "Epoch [212/300], Train Loss: 0.011422, Val Loss: 0.099015\n",
      "Epoch [213/300], Train Loss: 0.010796, Val Loss: 0.100293\n",
      "Epoch [214/300], Train Loss: 0.010536, Val Loss: 0.098291\n",
      "Epoch [215/300], Train Loss: 0.010515, Val Loss: 0.098586\n",
      "Epoch [216/300], Train Loss: 0.010365, Val Loss: 0.099918\n",
      "Epoch [217/300], Train Loss: 0.011139, Val Loss: 0.099255\n",
      "Epoch [218/300], Train Loss: 0.011802, Val Loss: 0.100121\n",
      "Epoch [219/300], Train Loss: 0.011635, Val Loss: 0.099573\n",
      "Epoch [220/300], Train Loss: 0.011196, Val Loss: 0.098749\n",
      "Epoch [221/300], Train Loss: 0.010810, Val Loss: 0.098497\n",
      "Epoch [222/300], Train Loss: 0.010338, Val Loss: 0.098328\n",
      "Epoch [223/300], Train Loss: 0.010165, Val Loss: 0.097836\n",
      "Epoch [224/300], Train Loss: 0.010065, Val Loss: 0.098372\n",
      "Epoch [225/300], Train Loss: 0.009795, Val Loss: 0.098452\n",
      "Epoch [226/300], Train Loss: 0.009801, Val Loss: 0.098024\n",
      "Epoch [227/300], Train Loss: 0.010095, Val Loss: 0.098705\n",
      "Epoch [228/300], Train Loss: 0.011228, Val Loss: 0.099179\n",
      "Epoch [229/300], Train Loss: 0.010786, Val Loss: 0.098138\n",
      "Epoch [230/300], Train Loss: 0.009840, Val Loss: 0.097851\n",
      "Epoch [231/300], Train Loss: 0.009603, Val Loss: 0.097281\n",
      "Epoch [232/300], Train Loss: 0.009426, Val Loss: 0.097272\n",
      "Epoch [233/300], Train Loss: 0.009345, Val Loss: 0.097576\n",
      "Epoch [234/300], Train Loss: 0.009707, Val Loss: 0.097881\n",
      "Epoch [235/300], Train Loss: 0.009885, Val Loss: 0.098197\n",
      "Epoch [236/300], Train Loss: 0.009671, Val Loss: 0.097603\n",
      "Epoch [237/300], Train Loss: 0.009591, Val Loss: 0.098036\n",
      "Epoch [238/300], Train Loss: 0.010633, Val Loss: 0.104916\n",
      "Epoch [239/300], Train Loss: 0.013411, Val Loss: 0.100807\n",
      "Epoch [240/300], Train Loss: 0.016767, Val Loss: 0.107169\n",
      "Epoch [241/300], Train Loss: 0.016234, Val Loss: 0.102320\n",
      "Epoch [242/300], Train Loss: 0.014002, Val Loss: 0.101238\n",
      "Epoch [243/300], Train Loss: 0.011045, Val Loss: 0.100002\n",
      "Epoch [244/300], Train Loss: 0.009677, Val Loss: 0.097807\n",
      "Epoch [245/300], Train Loss: 0.009561, Val Loss: 0.098691\n",
      "Epoch [246/300], Train Loss: 0.009540, Val Loss: 0.097506\n",
      "Epoch [247/300], Train Loss: 0.009347, Val Loss: 0.097528\n",
      "Epoch [248/300], Train Loss: 0.010838, Val Loss: 0.098918\n",
      "Epoch [249/300], Train Loss: 0.010358, Val Loss: 0.099475\n",
      "Epoch [250/300], Train Loss: 0.010004, Val Loss: 0.097897\n",
      "Epoch [251/300], Train Loss: 0.009135, Val Loss: 0.097044\n",
      "Epoch [252/300], Train Loss: 0.008756, Val Loss: 0.096876\n",
      "Epoch [253/300], Train Loss: 0.008546, Val Loss: 0.097210\n",
      "Epoch [254/300], Train Loss: 0.008664, Val Loss: 0.097209\n",
      "Epoch [255/300], Train Loss: 0.008597, Val Loss: 0.097238\n",
      "Epoch [256/300], Train Loss: 0.008794, Val Loss: 0.096855\n",
      "Epoch [257/300], Train Loss: 0.008531, Val Loss: 0.096781\n",
      "Epoch [258/300], Train Loss: 0.008511, Val Loss: 0.096848\n",
      "Epoch [259/300], Train Loss: 0.011332, Val Loss: 0.100752\n",
      "Epoch [260/300], Train Loss: 0.015198, Val Loss: 0.103642\n",
      "Epoch [261/300], Train Loss: 0.014324, Val Loss: 0.101233\n",
      "Epoch [262/300], Train Loss: 0.011164, Val Loss: 0.098608\n",
      "Epoch [263/300], Train Loss: 0.009520, Val Loss: 0.097586\n",
      "Epoch [264/300], Train Loss: 0.008810, Val Loss: 0.097141\n",
      "Epoch [265/300], Train Loss: 0.008758, Val Loss: 0.097182\n",
      "Epoch [266/300], Train Loss: 0.009919, Val Loss: 0.099373\n",
      "Epoch [267/300], Train Loss: 0.009813, Val Loss: 0.098363\n",
      "Epoch [268/300], Train Loss: 0.008987, Val Loss: 0.097890\n",
      "Epoch [269/300], Train Loss: 0.008608, Val Loss: 0.097394\n",
      "Epoch [270/300], Train Loss: 0.008554, Val Loss: 0.096659\n",
      "Epoch [271/300], Train Loss: 0.008211, Val Loss: 0.096623\n",
      "Epoch [272/300], Train Loss: 0.008024, Val Loss: 0.096385\n",
      "Epoch [273/300], Train Loss: 0.007921, Val Loss: 0.096471\n",
      "Epoch [274/300], Train Loss: 0.007953, Val Loss: 0.096231\n",
      "Epoch [275/300], Train Loss: 0.009071, Val Loss: 0.099535\n",
      "Epoch [276/300], Train Loss: 0.013428, Val Loss: 0.104984\n",
      "Epoch [277/300], Train Loss: 0.014307, Val Loss: 0.104702\n",
      "Epoch [278/300], Train Loss: 0.013208, Val Loss: 0.100274\n",
      "Epoch [279/300], Train Loss: 0.010513, Val Loss: 0.098799\n",
      "Epoch [280/300], Train Loss: 0.008622, Val Loss: 0.097293\n",
      "Epoch [281/300], Train Loss: 0.007952, Val Loss: 0.096668\n",
      "Epoch [282/300], Train Loss: 0.007938, Val Loss: 0.097187\n",
      "Epoch [283/300], Train Loss: 0.008138, Val Loss: 0.097288\n",
      "Epoch [284/300], Train Loss: 0.007953, Val Loss: 0.097007\n",
      "Epoch [285/300], Train Loss: 0.008059, Val Loss: 0.097080\n",
      "Epoch [286/300], Train Loss: 0.007947, Val Loss: 0.096632\n",
      "Epoch [287/300], Train Loss: 0.007679, Val Loss: 0.096162\n",
      "Epoch [288/300], Train Loss: 0.007661, Val Loss: 0.096333\n",
      "Epoch [289/300], Train Loss: 0.007723, Val Loss: 0.096804\n",
      "Epoch [290/300], Train Loss: 0.008229, Val Loss: 0.096564\n",
      "Epoch [291/300], Train Loss: 0.007974, Val Loss: 0.096848\n",
      "Epoch [292/300], Train Loss: 0.007942, Val Loss: 0.098429\n",
      "Epoch [293/300], Train Loss: 0.008796, Val Loss: 0.097745\n",
      "Epoch [294/300], Train Loss: 0.008064, Val Loss: 0.096705\n",
      "Epoch [295/300], Train Loss: 0.007738, Val Loss: 0.097097\n",
      "Epoch [296/300], Train Loss: 0.007537, Val Loss: 0.096152\n",
      "Epoch [297/300], Train Loss: 0.007544, Val Loss: 0.096629\n",
      "Epoch [298/300], Train Loss: 0.008130, Val Loss: 0.097161\n",
      "Epoch [299/300], Train Loss: 0.008414, Val Loss: 0.097545\n",
      "Epoch [300/300], Train Loss: 0.008198, Val Loss: 0.097111\n",
      "训练完成！\n",
      "验证集的平均损失: 0.096152\n"
     ]
    }
   ],
   "source": [
    "# 4. 定义LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        初始化LSTM模型。\n",
    "        \n",
    "        参数:\n",
    "        - input_size: 输入特征的数量（应变数据的特征数，例如10）\n",
    "        - hidden_size: LSTM隐藏层大小\n",
    "        - num_layers: LSTM层数\n",
    "        - output_size: 输出特征的数量（载荷数据的特征数，例如1）\n",
    "        - dropout: dropout概率\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # (num_layers, batch_size, hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # 前向传播LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # 通过全连接层\n",
    "        out = self.fc(out)  # (batch_size, seq_length, output_size)\n",
    "        return out\n",
    "\n",
    "# 初始化模型参数\n",
    "input_size = 10        # 应变数据的特征数量\n",
    "hidden_size = 1024     # LSTM隐藏层大小，可以根据需要调整\n",
    "num_layers = 4        # LSTM层数，可以根据需要调整\n",
    "output_size = 1        # 载荷数据的特征数量\n",
    "\n",
    "# 实例化模型\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "\n",
    "# 移动模型到GPU（如果可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 5. 训练与验证流程\n",
    "num_epochs = 300  # 根据需要调整\n",
    "best_val_loss = float('inf')\n",
    "patience = 100  # 早停耐心值\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (strain, load) in enumerate(train_loader):\n",
    "        strain = strain.to(device)  # 形状: (batch_size, 2048, 10)\n",
    "        load = load.to(device)      # 形状: (batch_size, 2048, 1)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(strain)     # 形状: (batch_size, 2048, 1)\n",
    "        loss = criterion(outputs, load)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * strain.size(0)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for strain, load in val_loader:\n",
    "            strain = strain.to(device)\n",
    "            load = load.to(device)\n",
    "            \n",
    "            outputs = model(strain)\n",
    "            loss = criterion(outputs, load)\n",
    "            val_loss += loss.item() * strain.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')\n",
    "    \n",
    "    # 早停检查\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # 保存最好的模型\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"早停触发！\")\n",
    "            break\n",
    "\n",
    "print(\"训练完成！\")\n",
    "\n",
    "# 6. 载入最好的模型（如果需要）\n",
    "model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "\n",
    "# 7. 评估模型\n",
    "model.eval()\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for strain, load in val_loader:\n",
    "        strain = strain.to(device)\n",
    "        load = load.to(device)\n",
    "        \n",
    "        outputs = model(strain)\n",
    "        loss = criterion(outputs, load)\n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "avg_test_loss = np.mean(test_losses)\n",
    "print(f'验证集的平均损失: {avg_test_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9f40d9-0d77-4670-9ce7-4ed88d6a001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存最佳模型，验证损失: 0.091581\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, 'best_lstm_model.pth')\n",
    "print(f'保存最佳模型，验证损失: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc803f6-ef17-4fc7-ac71-9f032cb37e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在适当的位置调用，比如每个训练周期结束后\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
